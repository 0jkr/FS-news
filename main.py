import os
import discord
from discord.ext import commands, tasks
from datetime import datetime
import aiohttp
import asyncio
from dotenv import load_dotenv
import feedparser
import random
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator

load_dotenv()

intents = discord.Intents.default()
# Ù†Ø³ØªØ®Ø¯Ù… slash commands ÙÙ‚Ø·ØŒ Ù„ÙƒÙ† command_prefix Ù…Ø·Ù„ÙˆØ¨
bot = commands.Bot(command_prefix='!', intents=intents)

DISCORD_TOKEN = os.getenv('DISCORD_TOKEN')
CHANNEL_NAME = 'ã€ŒğŸ“°ã€cyber-news'

# Ù…ØµØ§Ø¯Ø± RSS Ù„Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠØ© - ØªØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø®ØªØ±Ø§Ù‚Ø§Øª ÙˆØ§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
CYBER_NEWS_RSS_FEEDS = [
    'https://feeds.feedburner.com/TheHackersNews',  # Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø§Ø®ØªØ±Ø§Ù‚Ø§Øª ÙˆØ§Ù„Ø«ØºØ±Ø§Øª
    'https://www.bleepingcomputer.com/feed/',  # Ø§Ø®ØªØ±Ø§Ù‚Ø§Øª ÙˆØ«ØºØ±Ø§Øª Ø£Ù…Ù†ÙŠØ©
    'https://krebsonsecurity.com/feed/',  # Ø§Ø®ØªØ±Ø§Ù‚Ø§Øª Ù…Ù‡Ù…Ø©
    'https://www.darkreading.com/rss.xml',  # Ø«ØºØ±Ø§Øª ÙˆØ§Ø®ØªØ±Ø§Ù‚Ø§Øª Ø­Ø¯ÙŠØ«Ø©
    'https://feeds.feedburner.com/securityweek',  # Ø£Ø®Ø¨Ø§Ø± Ø£Ù…Ù†ÙŠØ© Ø­Ø¯ÙŠØ«Ø©
    'https://www.securityweek.com/rss',  # Ø«ØºØ±Ø§Øª ÙˆØ§Ø®ØªØ±Ø§Ù‚Ø§Øª
    'https://www.csoonline.com/index.rss',  # Ø£Ø®Ø¨Ø§Ø± Ø£Ù…Ù†ÙŠØ© ÙˆØ§Ø®ØªØ±Ø§Ù‚Ø§Øª
]

def clean_html(text):
    """ØªÙ†Ø¸ÙŠÙ HTML Ù…Ù† Ø§Ù„Ù†Øµ"""
    if not text:
        return ""
    soup = BeautifulSoup(text, 'html.parser')
    return soup.get_text().strip()

def translate_to_arabic(text):
    """ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    if not text or len(text.strip()) == 0:
        return text
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø©
        translator = GoogleTranslator(source='auto', target='ar')
        translated = translator.translate(text)
        return translated
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø©: {str(e)}")
        # ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø©ØŒ Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ
        return text

def extract_image_url(entry):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„Ø®Ø¨Ø±"""
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø­Ù‚ÙˆÙ„ Ù…Ø®ØªÙ„ÙØ©
    if 'media_content' in entry and entry['media_content']:
        return entry['media_content'][0].get('url', '')
    if 'media_thumbnail' in entry and entry['media_thumbnail']:
        return entry['media_thumbnail'][0].get('url', '')
    if 'links' in entry:
        for link in entry['links']:
            if link.get('type', '').startswith('image'):
                return link.get('href', '')
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ± ÙÙŠ HTML
    summary = entry.get('summary', entry.get('description', ''))
    if summary:
        soup = BeautifulSoup(summary, 'html.parser')
        img = soup.find('img')
        if img and img.get('src'):
            return img.get('src')
    
    return None

async def fetch_full_article(url):
    """Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Ø¥Ø²Ø§Ù„Ø© scripts Ùˆstyles
                    for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
                        script.decompose()
                    
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
                    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ tags Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰
                    content_tags = ['article', 'main', '.content', '.post-content', '.entry-content', 
                                   '.article-content', '#content', '.story-body']
                    
                    article_text = ""
                    for tag in content_tags:
                        if tag.startswith('.'):
                            elements = soup.select(tag)
                        elif tag.startswith('#'):
                            elements = soup.select(tag)
                        else:
                            elements = soup.find_all(tag)
                        
                        if elements:
                            for element in elements:
                                text = element.get_text(separator='\n', strip=True)
                                if len(text) > 200:  # Ù…Ø­ØªÙˆÙ‰ Ø°Ùˆ Ù…Ø¹Ù†Ù‰
                                    article_text = text
                                    break
                        
                        if article_text:
                            break
                    
                    # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù…Ø­ØªÙˆÙ‰ Ù…Ø­Ø¯Ø¯ØŒ Ù†Ø¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ
                    if not article_text or len(article_text) < 200:
                        article_text = soup.get_text(separator='\n', strip=True)
                    
                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
                    lines = article_text.split('\n')
                    cleaned_lines = []
                    for line in lines:
                        line = line.strip()
                        if line and len(line) > 20:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù‚ØµÙŠØ±Ø©
                            cleaned_lines.append(line)
                    
                    full_text = '\n\n'.join(cleaned_lines)
                    
                    # ØªÙ‚ØµÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ (Discord limit ~4000 chars)
                    if len(full_text) > 3500:
                        full_text = full_text[:3500] + "..."
                    
                    return full_text
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„ÙƒØ§Ù…Ù„: {str(e)}")
        return None
    
    return None

async def generate_cyber_news():
    """Ø¬Ù„Ø¨ Ø®Ø¨Ø± Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ Ù…Ù† RSS feeds ÙˆØªØ±Ø¬Ù…ØªÙ‡ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ù…Ø¬Ø§Ù†ÙŠ ØªÙ…Ø§Ù…Ø§Ù‹)"""
    # Ø§Ø®ØªÙŠØ§Ø± Ù…ØµØ¯Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ
    rss_url = random.choice(CYBER_NEWS_RSS_FEEDS)
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(rss_url, timeout=aiohttp.ClientTimeout(total=15)) as response:
                if response.status == 200:
                    content = await response.read()
                    feed = feedparser.parse(content)
                    
                    if feed.entries:
                        # ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù„Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø®ØªØ±Ø§Ù‚Ø§Øª ÙˆØ§Ù„Ø«ØºØ±Ø§Øª
                        keywords = ['hack', 'breach', 'vulnerability', 'exploit', 'zero-day', 'cyber attack', 
                                   'data breach', 'ransomware', 'malware', 'phishing', 'security flaw',
                                   'CVE', 'exploit', 'hacker', 'leak', 'compromise', 'intrusion']
                        
                        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø®Ø¨Ø§Ø± ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù‡Ù…Ø©
                        relevant_entries = []
                        for entry in feed.entries[:20]:
                            title = entry.get('title', '').lower()
                            summary = entry.get('summary', entry.get('description', '')).lower()
                            text = title + ' ' + summary
                            
                            if any(keyword in text for keyword in keywords):
                                relevant_entries.append(entry)
                        
                        # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ø£Ø®Ø¨Ø§Ø± Ø°Ø§Øª ØµÙ„Ø©ØŒ Ù†Ø®ØªØ§Ø± Ù…Ù†Ù‡Ø§ØŒ ÙˆØ¥Ù„Ø§ Ù†Ø®ØªØ§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹
                        if relevant_entries:
                            entry = random.choice(relevant_entries[:10])
                        else:
                            entry = random.choice(feed.entries[:10])
                        
                        title_en = entry.get('title', 'Cyber News')
                        description_en = entry.get('summary', entry.get('description', ''))
                        link = entry.get('link', '')
                        
                        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† HTML
                        title_en = clean_html(title_en)
                        description_en = clean_html(description_en)
                        
                        # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„
                        full_content = await fetch_full_article(link) if link else None
                        
                        # Ø¥Ø°Ø§ Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙˆØµÙ
                        if not full_content or len(full_content) < 200:
                            content_to_translate = description_en
                        else:
                            content_to_translate = full_content
                        
                        # ØªØ±Ø¬Ù…Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
                        title_ar = translate_to_arabic(title_en)
                        description_ar = translate_to_arabic(content_to_translate)
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø©
                        image_url = extract_image_url(entry)
                        
                        # Ø¥Ø±Ø¬Ø§Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø¨Ø±
                        return {
                            'title': title_ar,
                            'description': description_ar,
                            'link': link,
                            'image': image_url,
                            'title_en': title_en  # Ù„Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„ÙŠÙ‡ Ø¥Ø°Ø§ ÙØ´Ù„Øª Ø§Ù„ØªØ±Ø¬Ù…Ø©
                        }
                    else:
                        return None
                else:
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…ØµØ¯Ø± Ø¢Ø®Ø±
                    return await try_another_source()
    except asyncio.TimeoutError:
        return await try_another_source()
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø®Ø¨Ø± Ù…Ù† {rss_url}: {str(e)}")
        return await try_another_source()

async def try_another_source():
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ù…ØµØ¯Ø± Ø¢Ø®Ø± Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø£ÙˆÙ„"""
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…ØµØ¯Ø±ÙŠÙ† Ø¢Ø®Ø±ÙŠÙ†
    used_url = random.choice(CYBER_NEWS_RSS_FEEDS)
    remaining_feeds = [f for f in CYBER_NEWS_RSS_FEEDS if f != used_url]
    
    for rss_url in remaining_feeds[:2]:
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(rss_url, timeout=aiohttp.ClientTimeout(total=15)) as response:
                    if response.status == 200:
                        content = await response.read()
                        feed = feedparser.parse(content)
                        
                        if feed.entries:
                            # Ù†ÙØ³ Ø§Ù„ÙÙ„ØªØ±Ø© Ù„Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø®ØªØ±Ø§Ù‚Ø§Øª ÙˆØ§Ù„Ø«ØºØ±Ø§Øª
                            keywords = ['hack', 'breach', 'vulnerability', 'exploit', 'zero-day', 'cyber attack', 
                                       'data breach', 'ransomware', 'malware', 'phishing', 'security flaw',
                                       'CVE', 'exploit', 'hacker', 'leak', 'compromise', 'intrusion']
                            
                            relevant_entries = []
                            for entry in feed.entries[:20]:
                                title = entry.get('title', '').lower()
                                summary = entry.get('summary', entry.get('description', '')).lower()
                                text = title + ' ' + summary
                                
                                if any(keyword in text for keyword in keywords):
                                    relevant_entries.append(entry)
                            
                            if relevant_entries:
                                entry = random.choice(relevant_entries[:10])
                            else:
                                entry = random.choice(feed.entries[:10])
                            
                            title_en = clean_html(entry.get('title', 'Cyber News'))
                            description_en = clean_html(entry.get('summary', entry.get('description', '')))
                            link = entry.get('link', '')
                            
                            # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„
                            full_content = await fetch_full_article(link) if link else None
                            
                            # Ø¥Ø°Ø§ Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙˆØµÙ
                            if not full_content or len(full_content) < 200:
                                content_to_translate = description_en
                            else:
                                content_to_translate = full_content
                            
                            # ØªØ±Ø¬Ù…Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
                            title_ar = translate_to_arabic(title_en)
                            description_ar = translate_to_arabic(content_to_translate)
                            
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø©
                            image_url = extract_image_url(entry)
                            
                            return {
                                'title': title_ar,
                                'description': description_ar,
                                'link': link,
                                'image': image_url,
                                'title_en': title_en
                            }
        except:
            continue
    
    return None

async def send_news_to_channel():
    """Ø¥Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ù†Ø§Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©"""
    try:
        news = await generate_cyber_news()
        
        if not news:
            print(f"[{datetime.now()}] âš ï¸ Ù„Ù… ÙŠØªÙ… Ø¬Ù„Ø¨ Ø®Ø¨Ø±")
            return False
        
        for guild in bot.guilds:
            for channel in guild.channels:
                if isinstance(channel, discord.TextChannel) and CHANNEL_NAME in channel.name:
                    embed = discord.Embed(
                        title=f"ğŸ“° {news.get('title', 'Ø®Ø¨Ø± Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ')}",
                        description=news.get('description', 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ'),
                        color=discord.Color.blue(),
                        timestamp=datetime.now()
                    )
                    
                    # Ø¥Ø¶Ø§ÙØ© Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„
                    if news.get('link'):
                        embed.url = news['link']
                        embed.add_field(name="ğŸ”—", value=f"[Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯]({news['link']})", inline=False)
                    
                    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØµÙˆØ±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªÙˆÙØ±Ø©
                    if news.get('image'):
                        embed.set_image(url=news['image'])
                    
                    await channel.send(embed=embed)
                    print(f"[{datetime.now()}] ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø¥Ù„Ù‰ {channel.name} ÙÙŠ {guild.name}")
                    return True
        
        print(f"[{datetime.now()}] âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ù†Ø§Ø© '{CHANNEL_NAME}'")
        return False
    except Exception as e:
        print(f"[{datetime.now()}] âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø®Ø¨Ø±: {str(e)}")
        return False

@bot.event
async def on_ready():
    print(f'{bot.user} ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¨Ù†Ø¬Ø§Ø­!')
    print(f'Ø§Ù„Ø¨ÙˆØª Ù…ØªØµÙ„ Ø¨Ù€ {len(bot.guilds)} Ø³ÙŠØ±ÙØ±')
    
    # Ù…Ø²Ø§Ù…Ù†Ø© slash commands
    try:
        synced = await bot.tree.sync()
        print(f'âœ… ØªÙ… Ù…Ø²Ø§Ù…Ù†Ø© {len(synced)} Ø£Ù…Ø± slash')
    except Exception as e:
        print(f'âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø£ÙˆØ§Ù…Ø±: {e}')
    
    # Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„Ø©
    if not send_news_periodically.is_running():
        send_news_periodically.start()

# Slash command - Ù„Ø§ ÙŠØ­ØªØ§Ø¬ MESSAGE_CONTENT_INTENT
@bot.tree.command(name="ÙˆØ±ÙŠÙ†ÙŠ", description="Ø¥Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ ÙÙˆØ±ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
async def show_news(interaction: discord.Interaction):
    """Ø¥Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ ÙÙˆØ±ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
    try:
        await interaction.response.defer()
        news = await generate_cyber_news()
        
        if not news:
            await interaction.followup.send("âŒ ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø§Ù„Ø®Ø¨Ø±. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.")
            return
        
        embed = discord.Embed(
            title=f"ğŸ“° {news.get('title', 'Ø®Ø¨Ø± Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ')}",
            description=news.get('description', 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ'),
            color=discord.Color.green(),
            timestamp=datetime.now()
        )
        
        # Ø¥Ø¶Ø§ÙØ© Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„
        if news.get('link'):
            embed.url = news['link']
            embed.add_field(name="ğŸ”—", value=f"[Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯]({news['link']})", inline=False)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØµÙˆØ±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªÙˆÙØ±Ø©
        if news.get('image'):
            embed.set_image(url=news['image'])
        
        await interaction.followup.send(embed=embed)
    except Exception as e:
        await interaction.followup.send(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")

@tasks.loop(hours=6)
async def send_news_periodically():
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙƒÙ„ 6 Ø³Ø§Ø¹Ø§Øª"""
    print(f"[{datetime.now()}] Ø¬Ø§Ø±ÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ Ù…Ø¬Ø¯ÙˆÙ„...")
    await send_news_to_channel()

@send_news_periodically.before_loop
async def before_send_news():
    await bot.wait_until_ready()

if __name__ == "__main__":
    if not DISCORD_TOKEN:
        print("âŒ Ø®Ø·Ø£: DISCORD_TOKEN ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…Ù„Ù .env")
        print("   Ø£Ø¶Ù DISCORD_TOKEN ÙÙŠ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø¹Ù„Ù‰ Railway")
    else:
        try:
            print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª...")
            print("ğŸ“° Ø§Ù„Ø¨ÙˆØª ÙŠØ³ØªØ®Ø¯Ù… Ù…ØµØ§Ø¯Ø± RSS Ù…Ø¬Ø§Ù†ÙŠØ© Ù„Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠØ©")
            bot.run(DISCORD_TOKEN)
        except Exception as e:
            print(f"\nâŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
            print(f"   Ù†ÙˆØ¹ Ø§Ù„Ø®Ø·Ø£: {type(e).__name__}")
            raise

